{
    "code": "import os\nimport pandas as pd\nimport requests\nimport io\n\n# Define the output directory and create it if it doesn't exist\noutput_dir = \"uploads/470566af-a18d-47ab-96f9-a62357ec6757\"\nos.makedirs(output_dir, exist_ok=True)\n\n# URL of the Wikipedia page to scrape\nurl = \"https://en.wikipedia.org/wiki/List_of_highest-grossing_films\"\n\n# Send a GET request to the URL\nresponse = requests.get(url)\nresponse.raise_for_status()  # Raise an exception for bad status codes\n\n# Use pandas to read the HTML tables from the page content\n# We are interested in the first table with the class 'wikitable'\ntables = pd.read_html(response.text, attrs={\"class\": \"wikitable\"})\ndf = tables[0]\n\n# --- Data Cleaning ---\n\n# Rename columns for easier access\ndf.columns = ['Rank', 'Peak', 'Title', 'Worldwide_gross', 'Year', 'Reference']\n\n# Clean the 'Worldwide_gross' column\n# 1. Remove bracketed citations (e.g., [1], [2])\n# 2. Remove currency symbols ('$') and commas (',')\n# 3. Convert the cleaned string to a numeric (float) type\ndf['Worldwide_gross'] = df['Worldwide_gross'].astype(str).str.replace(r'\\[.*?\\]', '', regex=True)\ndf['Worldwide_gross'] = df['Worldwide_gross'].str.replace(r'[$,]', '', regex=True).astype(float)\n\n# Convert 'Rank', 'Peak', and 'Year' columns to numeric types\n# 'coerce' will turn any non-numeric values into NaN (Not a Number)\ndf['Rank'] = pd.to_numeric(df['Rank'], errors='coerce')\ndf['Peak'] = pd.to_numeric(df['Peak'], errors='coerce')\ndf['Year'] = pd.to_numeric(df['Year'], errors='coerce')\n\n# Drop rows that have NaN values in key columns after conversion\ndf.dropna(subset=['Rank', 'Peak', 'Year', 'Worldwide_gross'], inplace=True)\n\n# Convert the numeric columns to integer types for cleaner data\ndf['Rank'] = df['Rank'].astype(int)\ndf['Peak'] = df['Peak'].astype(int)\ndf['Year'] = df['Year'].astype(int)\n\n# --- Save Data and Metadata ---\n\n# Define the path for the output CSV file\ndata_path = os.path.join(output_dir, \"data.csv\")\n# Save the cleaned DataFrame to the CSV file, without the pandas index\ndf.to_csv(data_path, index=False)\n\n# Define the path for the metadata file\nmetadata_path = os.path.join(output_dir, \"metadata.txt\")\n\n# Capture the DataFrame's info (column types, non-null counts) into a string buffer\nbuffer = io.StringIO()\ndf.info(buf=buffer)\ninfo_str = buffer.getvalue()\n\n# Get the list of column names\ncolumn_names = df.columns.tolist()\n\n# Get the first few rows of the DataFrame as a string\nhead_str = df.head().to_string()\n\n# The original question specified the answer format\nanswer_format = \"JSON array\"\n\n# Assemble the complete metadata content\nmetadata_content = f\"\"\"path: {data_path}\n\n{info_str}\ncolumns: {column_names}\n\n{head_str}\n\nANSWER_FORMAT: {answer_format}\n\"\"\"\n\n# Write the metadata content to the metadata.txt file\nwith open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(metadata_content)\n\nprint(f\"Data saved to {data_path}\")\nprint(f\"Metadata saved to {metadata_path}\")\n",
    "libraries": [
        "pandas",
        "requests",
        "lxml"
    ],
    "questions": [
        "How many $2 bn movies before 2023?",
        "Earliest film over $1.5 bn?",
        "Correlation between Rank and Peak?",
        "Scatterplot of Rank vs Peak with dotted red regression line as base64 PNG."
    ],
    "comment": "Step-3: Getting scrap code and metadata from llm. Tries count = %d 0"
}