{
    "code": "import os\nimport pandas as pd\nimport requests\nfrom io import StringIO\n\n# Define the output directory\noutput_dir = 'uploads/bf9b0b8e-f6bf-4c94-8101-0e2a96bb95f4'\nos.makedirs(output_dir, exist_ok=True)\n\n# Define file paths\ndata_path = os.path.join(output_dir, 'data.csv')\nmetadata_path = os.path.join(output_dir, 'metadata.txt')\n\n# URL of the Wikipedia page\nurl = \"https://en.wikipedia.org/wiki/List_of_highest-grossing_films\"\n\n# Fetch the HTML content\nheaders = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'}\nresponse = requests.get(url, headers=headers)\nhtml_content = response.content\n\n# Read the HTML tables into a list of DataFrames\n# The first table is the one for highest-grossing films.\ntables = pd.read_html(html_content)\ndf = tables[0]\n\n# --- Data Cleaning ---\n# Rename columns for easier access\ndf.columns = ['Rank', 'Peak', 'Title', 'Worldwide_gross', 'Year', 'Reference']\n\n# Clean the 'Worldwide_gross' column\ndf['Worldwide_gross'] = df['Worldwide_gross'].astype(str).str.replace(r'\\s*\\(USD\\)', '', regex=True) # remove (USD)\ndf['Worldwide_gross'] = df['Worldwide_gross'].astype(str).str.replace(r'\\[.*?\\]', '', regex=True) # remove references\ndf['Worldwide_gross'] = df['Worldwide_gross'].str.replace('$', '', regex=False).str.replace(',', '', regex=False)\ndf['Worldwide_gross'] = pd.to_numeric(df['Worldwide_gross'], errors='coerce')\n\n# Clean the 'Year' column\ndf['Year'] = df['Year'].astype(str).str.extract(r'(\\d{4})', expand=False)\ndf['Year'] = pd.to_numeric(df['Year'], errors='coerce')\n\n# Clean Rank and Peak to be numeric\ndf['Rank'] = pd.to_numeric(df['Rank'], errors='coerce')\ndf['Peak'] = pd.to_numeric(df['Peak'], errors='coerce')\n\n# Drop rows with NaN values in crucial columns\ndf.dropna(subset=['Worldwide_gross', 'Year', 'Rank', 'Peak'], inplace=True)\n\n# Convert columns to appropriate integer types\ndf = df.astype({'Rank': 'int64', 'Peak': 'int64', 'Worldwide_gross': 'int64', 'Year': 'int64'})\n\n# Save the cleaned data to a CSV file\ndf.to_csv(data_path, index=False)\n\n# --- Metadata Generation ---\nwith open(metadata_path, 'w', encoding='utf-8') as f:\n    f.write(f\"path: {data_path}\\n\")\n    f.write(f\"description: Table of highest-grossing films scraped from Wikipedia.\\n\\n\")\n\n    # Capture df.info() output\n    buffer = StringIO()\n    df.info(buf=buffer)\n    f.write(\"df.info():\\n\")\n    f.write(buffer.getvalue())\n    f.write(\"\\n\")\n\n    f.write(\"Column names:\\n\")\n    f.write(f\"{df.columns.tolist()}\\n\\n\")\n\n    f.write(\"First few rows (df.head()):\\n\")\n    f.write(df.head().to_string())\n    f.write(\"\\n\\n\")\n\n    # Based on the user's request \"Answer as JSON array\"\n    f.write(\"ANSWER_FORMAT: JSON\\n\")\n\nprint(f\"Data saved to {data_path}\")\nprint(f\"Metadata saved to {metadata_path}\")\n",
    "libraries": [
        "pandas",
        "requests",
        "lxml"
    ],
    "questions": [
        "How many $2 bn movies before 2023?",
        "Earliest film over $1.5 bn?",
        "Correlation between Rank and Peak?",
        "Scatterplot of Rank vs Peak with dotted red regression line as base64 PNG."
    ],
    "comment": "Step-3: Getting scrap code and metadata from llm. Tries count = %d 0"
}